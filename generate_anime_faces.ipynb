{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Face Generation using [Generative Adverserial Networks (GANs)](https://en.wikipedia.org/wiki/Generative_adversarial_network)\n",
    "\n",
    "**Faris Hijazi**\n",
    "\n",
    "This is a fork of the [Udacity deep learning with Pytorch nanodegree](https://www.udacity.com/course/deep-learning-pytorch--ud188) [DCGAN face generation project](https://github.com/udacity/deep-learning-v2-pytorch/tree/master/project-face-generation).\n",
    "\n",
    "However some heavy modifictions and **updgrades** have been made, such as extending the GANs and adding support for checkpoints as well as connecting with [Tensorboard](https://www.tensorflow.org/tensorboard)\n",
    "\n",
    "In this project. The goal is to get a generator network to generate *new* images of faces that look as realistic as possible.\n",
    "\n",
    "<!--\n",
    "### Get the Data\n",
    "\n",
    "This is a zip file that you'll need to extract in the home directory of this notebook for further loading and processing. After extracting the data, you should be left with a directory of data `processed_celeba_small/`\n",
    "-->\n",
    "\n",
    "## Todo list:\n",
    "\n",
    "- [x] automate the creation of neural networks given depth parameter\n",
    "- [x] connect to tensorboard\n",
    "- [ ] add attention maps\n",
    "- [ ] use labels to improve training\n",
    "\n",
    "Some resources:\n",
    "- https://github.com/soumith/ganhacks\n",
    "- [batch norm](https://gist.github.com/shagunsodhani/4441216a298df0fe6ab0)\n",
    "- https://sites.google.com/view/cvpr2018tutorialongans/\n",
    "- this guys makes GANs using keras and it's so simple! CHECK IT!!! https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:34.789188Z",
     "start_time": "2019-12-24T12:20:34.108885Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import time\n",
    "from collections import UserDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import ImageFile\n",
    "from torchvision import datasets, transforms\n",
    "import ipywidgets\n",
    "\n",
    "import unittests as tests\n",
    "import utils\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:34.810168Z",
     "start_time": "2019-12-24T12:20:34.797179Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose hyperparams\n",
    "\n",
    "img_size = (64, 64, 3)\n",
    "\n",
    "# directories should be followed by a \"/\"\n",
    "data_dir = \"D:/Buza/Projects/datasets/anime-faces/\"\n",
    "\n",
    "###\n",
    "# just making paths and performing checks\n",
    "assert os.path.isdir(data_dir)\n",
    "data_dir_name = os.path.split(os.path.split(data_dir)[0])[-1]\n",
    "data_name = '{}-{}'.format(data_dir_name, 'x'.join(map(str, img_size[0:2]))).replace(' ', '_')\n",
    "run_dir = f'./runs/{data_name}'\n",
    "\n",
    "print(f'Data directory: \"{data_name}\"')\n",
    "os.makedirs(os.path.join(run_dir, 'samples'), exist_ok=True)\n",
    "os.makedirs(os.path.join(run_dir, 'checkpoints'), exist_ok=True)\n",
    "\n",
    "\n",
    "hyperparams = {}\n",
    "if os.path.exists(os.path.join(run_dir, 'hyperparams.json')):\n",
    "    with open(os.path.join(run_dir, 'hyperparams.json'), 'r') as f:\n",
    "        hyperparams = json.load(f)\n",
    "        print(f'Found existing hyperparams in {run_dir}:', hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:34.796182Z",
     "start_time": "2019-12-24T12:20:34.790187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model hyperparams\n",
    "\n",
    "# hyperparams = {\n",
    "#     'd_conv_dim': 256,\n",
    "#     'g_conv_dim': 256,\n",
    "#     'n_layers': 4,\n",
    "#     'z_size': 128,\n",
    "#     'batch_size': 64,\n",
    "# }\n",
    "\n",
    "# hyperparams = {\n",
    "#     'd_conv_dim': 64,\n",
    "#     'g_conv_dim': 64,\n",
    "#     'n_layers': 2,\n",
    "#     'z_size': 50,\n",
    "#     'batch_size': 64,\n",
    "# }\n",
    "\n",
    "hyperparams = {\n",
    "    'd_conv_dim': 128,\n",
    "    'g_conv_dim': 128,\n",
    "    'n_layers': 5,\n",
    "    'z_size': 128,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "\n",
    "with open(os.path.join(run_dir, 'hyperparams.json'), 'w') as f:\n",
    "    json.dump(hyperparams, f, indent=4)\n",
    "\n",
    "\n",
    "d_conv_dim = hyperparams['d_conv_dim']\n",
    "g_conv_dim = hyperparams['g_conv_dim']\n",
    "n_layers = hyperparams['n_layers']\n",
    "z_size = hyperparams['z_size']\n",
    "batch_size = hyperparams['batch_size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "<!-- The [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations, you'll only need the images. -->\n",
    "Note that these are color images with [3 color channels (RGB)](https://en.wikipedia.org/wiki/Channel_(digital_image)#RGB_Images) each.\n",
    "\n",
    "### Pre-process and Load the Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you.\n",
    "Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. This *pre-processed* dataset is a smaller subset of the very large CelebA data.\n",
    "\n",
    "<!-- #### ImageFolder\n",
    "\n",
    "To create a dataset given a directory of images, it's recommended that you use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) wrapper, with a root directory `processed_celeba_small/` and data transformation passed in. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataLoader\n",
    "\n",
    "#### Exercise: Create a DataLoader anime `train_loader` with appropriate hyperparameters.\n",
    "\n",
    "Call the above function and create a dataloader to view images. \n",
    "* You can decide on any reasonable `batch_size` parameter\n",
    "* Your `image_size` **must be** `32`. Resizing the data to a smaller size will make for faster training, while still creating convincing images of faces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:35.197770Z",
     "start_time": "2019-12-24T12:20:34.811166Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, img_size, data_dir=data_dir, num_workers=0):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param batch_size: The size of each batch; the number of images in a batch\n",
    "    :param img_size: The square size of the image data (x, y)\n",
    "    :param data_dir: Directory where image data is located\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.Compose([\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomRotation(30),\n",
    "        transforms.Resize(img_size[0:2]),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # get the training datasets\n",
    "    train_data = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "    # prepare data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                               num_workers=num_workers)\n",
    "    \n",
    "    return train_loader\n",
    "\n",
    "\n",
    "# Call your function and get a dataloader\n",
    "train_loader = get_dataloader(batch_size, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can view some images! You should seen square images of somewhat-centered faces.\n",
    "\n",
    "Note: You'll need to convert the Tensor images into a NumPy type and transpose the dimensions to correctly display an image, suggested `imshow` code is below, but it may not be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:36.058890Z",
     "start_time": "2019-12-24T12:20:35.198769Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# helper display function\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "for i in range(7):\n",
    "    dataiter.next()\n",
    "\n",
    "images, _ = dataiter.next() # _ for no labels\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Pre-process your image data and scale it to a pixel range of -1 to 1\n",
    "\n",
    "You need to do a bit of pre-processing; you know that the output of a `tanh` activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:36.070877Z",
     "start_time": "2019-12-24T12:20:36.060888Z"
    }
   },
   "outputs": [],
   "source": [
    "# check scaled range\n",
    "# should be close to -1 to 1\n",
    "img = images[0]\n",
    "scaled_img = utils.scale(img)\n",
    "\n",
    "print('Min: ', scaled_img.min())\n",
    "print('Max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:36.077870Z",
     "start_time": "2019-12-24T12:20:36.071877Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload;reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model\n",
    "\n",
    "A GAN is comprised of two adversarial networks, a discriminator and a generator.\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "Your first task will be to define the discriminator. This is a convolutional classifier like you've built before, only without any maxpooling layers. To deal with this complex data, it's suggested you use a deep network with **normalization**. You are also allowed to create any helper functions that may be useful.\n",
    "\n",
    "#### Exercise: Complete the Discriminator class\n",
    "* The inputs to the discriminator are 32x32x3 tensor images\n",
    "* The output should be a single value that will indicate whether a given image is real or fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:37.640273Z",
     "start_time": "2019-12-24T12:20:37.576338Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 conv_dim=32,\n",
    "                 n_layers=4,\n",
    "                 dropout_prob=0.5,\n",
    "                 n_classes=1,\n",
    "                 in_shape=(32, 32, 3)):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv_dim = conv_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.in_shape = in_shape\n",
    "\n",
    "        self.convs = [None] * self.n_layers\n",
    "        # starting with last layer\n",
    "        for i in range(self.n_layers, 0, -1):\n",
    "            layer_num = self.n_layers - i + 1\n",
    "            batch_norm = True\n",
    "            in_channels = conv_dim * (2**(layer_num - 2))\n",
    "            out_channels = conv_dim * (2**(layer_num - 1))\n",
    "\n",
    "            if i == self.n_layers:\n",
    "                batch_norm = False  # batch_norm for last layer only\n",
    "                in_channels = in_shape[2]\n",
    "\n",
    "            conv_ = utils.conv(in_channels,\n",
    "                               out_channels,\n",
    "                               4,\n",
    "                               batch_norm=batch_norm)\n",
    "            self.convs[layer_num - 1] = conv_\n",
    "            setattr(self, f'conv{layer_num}', conv_)\n",
    "\n",
    "\n",
    "#             print(f'conv{layer_num}', conv_)\n",
    "\n",
    "        assert len(self.convs) == self.n_layers\n",
    "\n",
    "        #         # complete init function\n",
    "        #         # in 32x32x3\n",
    "        #         self.conv1 = utils.conv(3,          conv_dim,   4, stride=2, batch_norm=False)\n",
    "        #         # out 16x16x(conv_dim)\n",
    "        #         self.conv2 = utils.conv(conv_dim,   conv_dim*2, 4, stride=2, batch_norm=True)\n",
    "        #         # out 8x8x(conv_dim*2)\n",
    "        #         self.conv3 = utils.conv(conv_dim*2, conv_dim*4, 4, stride=2, batch_norm=True)\n",
    "        #         # out 4x4x(conv_dim*4)\n",
    "        #         self.conv4 = utils.conv(conv_dim*4, conv_dim*8, 4, stride=2, batch_norm=True)\n",
    "        #         # out 2x2x(conv_dim*8)\n",
    "\n",
    "        #         print('convs', self.convs)\n",
    "\n",
    "        self.conv_out_size = (\n",
    "            conv_dim * (2**(n_layers - 1)),\n",
    "            in_shape[0] // (2**(n_layers)),\n",
    "            in_shape[1] // (2**(n_layers)),\n",
    "        )\n",
    "        self.fc = nn.Linear(np.prod(self.conv_out_size), n_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.apply(utils.weights_init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: size(batch_size, 32, 32, 3)\n",
    "        x = x.to(self.get_device())\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # starting with last layer\n",
    "        for i in range(1, self.n_layers + 1, 1):\n",
    "            conv_ = getattr(self, f'conv{i}')\n",
    "            x = self.leaky_relu(self.dropout(conv_(x)))\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(batch_size, np.prod(self.conv_out_size))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def get_device(self):\n",
    "        dev = next(self.parameters()).get_device()\n",
    "        dev = 'cpu' if dev == -1 else f'cuda:{dev}'\n",
    "        return dev\n",
    "\n",
    "tests.test_discriminator(Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The generator should upsample an input and generate a *new* image of the same size as our training data `32x32x3`. This should be mostly transpose convolutional layers with normalization applied to the outputs.\n",
    "\n",
    "#### Exercise: Complete the Generator class\n",
    "* The inputs to the generator are vectors of some length `z_size`\n",
    "* The output should be a image of shape `32x32x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:37.574339Z",
     "start_time": "2019-12-24T12:20:36.078869Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 z_size,\n",
    "                 conv_dim=32,\n",
    "                 n_layers=4,\n",
    "                 out_shape=(32, 32, 3),\n",
    "                 dropout_prob=0.5):\n",
    "        \"\"\"Initialize the Generator Module\n",
    "        :param z_size: The length of the input latent vector, z\n",
    "        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # complete init function\n",
    "        self.conv_dim = conv_dim\n",
    "        self.z_size = z_size\n",
    "        self.n_layers = n_layers\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "        self.conv_in_size = (\n",
    "            conv_dim * (2**(n_layers - 1)),\n",
    "            out_shape[0] // (2**(n_layers)),\n",
    "            out_shape[1] // (2**(n_layers)),\n",
    "        )\n",
    "        #         print('self.conv_in_size', self.conv_in_size)\n",
    "\n",
    "        self.fc = nn.Linear(z_size, np.prod(self.conv_in_size))\n",
    "\n",
    "        self.deconvs = [None] * self.n_layers\n",
    "\n",
    "        # starting with last layer\n",
    "        for i in range(1, self.n_layers + 1, 1):\n",
    "            layer_num = self.n_layers - i + 1\n",
    "            batch_norm = True\n",
    "            in_channels = conv_dim * (2**(i - 1))\n",
    "            out_channels = conv_dim * (2**(i - 2))\n",
    "\n",
    "            if i == 1:\n",
    "                batch_norm = False  # batch_norm for last layer only\n",
    "                out_channels = out_shape[2]\n",
    "\n",
    "            deconv_ = utils.deconv(in_channels,\n",
    "                                   out_channels,\n",
    "                                   4,\n",
    "                                   batch_norm=batch_norm)\n",
    "            self.deconvs[layer_num - 1] = deconv_\n",
    "            setattr(self, f'deconv{layer_num}', deconv_)\n",
    "#             print(f'deconv{layer_num}', deconv_)\n",
    "\n",
    "        assert len(self.deconvs) == self.n_layers\n",
    "\n",
    "        #         # in 2x2x(conv_dim*8)\n",
    "        #         self.deconv1 = utils.deconv(conv_dim*8, conv_dim*4, 4)\n",
    "        #         # in 4x4x(conv_dim*4)\n",
    "        #         self.deconv2 = utils.deconv(conv_dim*4, conv_dim*2, 4)\n",
    "        #         # out 8x8x(conv_dim*2)\n",
    "        #         self.deconv3 = utils.deconv(conv_dim*2, conv_dim,   4)\n",
    "        #         # out 16x16x(conv_dim)\n",
    "        #         self.deconv4 = utils.deconv(conv_dim,   3,          4, batch_norm=False)\n",
    "        #         # out 32x32x3\n",
    "\n",
    "        #         print('deconvs', self.deconvs)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.apply(utils.weights_init_normal)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward propagation of the neural network\n",
    "        :param x: The input to the neural network     \n",
    "        :return: A 32x32x3 Tensor image as output\n",
    "        \"\"\"\n",
    "        x = x.to(self.get_device())\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch_size, *self.conv_in_size)\n",
    "\n",
    "        # starting with last layer\n",
    "        for i in range(1, self.n_layers + 1, 1):\n",
    "            deconv_ = getattr(self, f'deconv{i}')\n",
    "            x = deconv_(x)\n",
    "            if i != self.n_layers:  # if not last layer\n",
    "                x = self.leaky_relu(self.dropout(x))\n",
    "\n",
    "\n",
    "#         x = self.leaky_relu(self.dropout(self.deconv1(x)))\n",
    "#         x = self.leaky_relu(self.dropout(self.deconv2(x)))\n",
    "#         x = self.leaky_relu(self.dropout(self.deconv3(x)))\n",
    "#         x = self.deconv4(x)\n",
    "\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    def get_device(self):\n",
    "        dev = next(self.parameters()).get_device()\n",
    "        dev = 'cpu' if dev == -1 else f'cuda:{dev}'\n",
    "        return dev\n",
    "\n",
    "    def generate(self, batch_size):\n",
    "        return self(Generator.gen_z(batch_size, self.z_size))\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_z(batch_size, z_size):\n",
    "        return torch.from_numpy(np.random.normal(size=(batch_size, z_size))).float()\n",
    "\n",
    "tests.test_generator(Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights of your networks\n",
    "\n",
    "To help your models converge, you should initialize the weights of the convolutional and linear layers in your model. From reading the [original DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf), they say:\n",
    "> All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.\n",
    "\n",
    "So, your next task will be to define a weight initialization function that does just this!\n",
    "\n",
    "You can refer back to the lesson on weight initialization or even consult existing model code, such as that from [the `networks.py` file in CycleGAN Github repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py) to help you complete this function.\n",
    "\n",
    "#### Exercise: Complete the weight initialization function\n",
    "\n",
    "* This should initialize only **convolutional** and **linear** layers\n",
    "* Initialize the weights to a normal distribution, centered around 0, with a standard deviation of 0.02.\n",
    "* The bias terms, if they exist, may be left alone or set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build complete network\n",
    "\n",
    "Define your models' hyperparameters and instantiate the discriminator and generator from the classes defined above. Make sure you've passed in the correct input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:37.654257Z",
     "start_time": "2019-12-24T12:20:37.642270Z"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = 1\n",
    "D = Discriminator(           conv_dim=d_conv_dim, n_layers=n_layers, in_shape=img_size, n_classes=n_classes)\n",
    "G = Generator(z_size=z_size, conv_dim=g_conv_dim, n_layers=n_layers, out_shape=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G,'\\n\\n', D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:37.663248Z",
     "start_time": "2019-12-24T12:20:37.656255Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'#remove me\n",
    "if device == 'cuda':\n",
    "    print(f\"USING {torch.cuda.device_count()} GPU!\")\n",
    "\n",
    "dev0, dev1 = 'cuda', 'cuda'\n",
    "# if device == 'cuda' and torch.cuda.device_count() > 1:\n",
    "#     dev0, dev1 = 'cuda:0', 'cuda:1'\n",
    "\n",
    "# move models to GPU\n",
    "# D.to(dev0)\n",
    "# G.to(dev1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "#### Exercise: Define optimizers for your Discriminator (D) and Generator (G)\n",
    "\n",
    "Define optimizers for your models with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried beta1=0.1 and beta2=0.9 but the discriminator kept overwhelming the generator, we need a good generator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:37.671241Z",
     "start_time": "2019-12-24T12:20:37.665247Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "beta1 = 0.3\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create optimizers for the discriminator and generator\n",
    "d_optimizer = optim.SGD(D.parameters(), lr=0.0002, momentum=0.5)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0001, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses for both types of adversarial networks.\n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "> * For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`. \n",
    "* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "The generator loss will look similar only with flipped labels. The generator's goal is to get the discriminator to *think* its generated images are *real*.\n",
    "\n",
    "#### Exercise: Complete real and fake loss functions\n",
    "\n",
    "**You may choose to use either cross entropy or a least squares error loss to complete the following `real_loss` and `fake_loss` functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:37.683229Z",
     "start_time": "2019-12-24T12:20:37.673239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "def real_loss(D_out, labels=None, smooth=False):\n",
    "    \"\"\"Calculates how close discriminator outputs are to being real.\n",
    "       :param, D_out: discriminator logits\n",
    "       :param labels: \n",
    "       :param smooth: \n",
    "       :return: real loss\"\"\"\n",
    "\n",
    "    #     dev = D_out.get_device()\n",
    "    #     device = dev = 'cpu' if dev == -1 else f'cuda:{dev}'\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if labels is not None:\n",
    "        # labels passed\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        # labels are 1's\n",
    "        labels = torch.ones(D_out.size(0))\n",
    "        D_out = D_out.squeeze()\n",
    "\n",
    "    if smooth:\n",
    "        labels = np.abs(labels -\n",
    "                        torch.from_numpy(np.abs(np.random.normal(scale=0.1, size=labels.shape))))\n",
    "\n",
    "    return criterion(D_out, labels.to(D_out.to(device)))\n",
    "\n",
    "\n",
    "def fake_loss(D_out, device=device):\n",
    "    \"\"\"Calculates how close discriminator outputs are to being fake.\n",
    "       param, D_out: discriminator logits\n",
    "       return: fake loss\"\"\"\n",
    "    #     dev = D_out.get_device()\n",
    "    #     device = dev = 'cpu' if dev == -1 else f'cuda:{dev}'\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    labels = torch.zeros(D_out.size(0))\n",
    "\n",
    "    return criterion(D_out.squeeze(), labels.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator. You'll use your functions `real_loss` and `fake_loss` to help you calculate the discriminator losses.\n",
    "\n",
    "* You should train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and should have an opposing loss function\n",
    "\n",
    "\n",
    "#### Saving Samples\n",
    "\n",
    "You've been given some code to print out some loss statistics and save some generated \"fake\" samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:38.143758Z",
     "start_time": "2019-12-24T12:20:37.685226Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import summary\n",
    "import subprocess\n",
    "\n",
    "port = 6006\n",
    "args_tb = [\"tensorboard\", f'--logdir=\"{run_dir}\"', f\"--port={port}\", \"--samples_per_plugin=scalars=5000,images=200\"]\n",
    "print('Run:\\n', ' '.join(args_tb))\n",
    "# subprocess.Popen(args_tb)\n",
    "# subprocess.Popen([\"run_tensorboard.bat\"])\n",
    "\n",
    "print(f'Launch tensorboard and visit:\\n http://localhost:{port}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:21:17.979015Z",
     "start_time": "2019-12-24T12:21:09.712Z"
    }
   },
   "outputs": [],
   "source": [
    "# checking if a checkpoint exists\n",
    "ckpt = utils.load_ckpt(os.path.join(run_dir, 'checkpoints'))\n",
    "if ckpt is not None:\n",
    "    print(list(ckpt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session\n",
    "from utils import dict_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:21:18.007985Z",
     "start_time": "2019-12-24T12:21:10.552Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(D,\n",
    "          G,\n",
    "          n_epochs,\n",
    "          print_every=100,\n",
    "          save_every=100,\n",
    "          device='cpu',\n",
    "          path=run_dir,\n",
    "          ckpt=True,\n",
    "          load_models=True,\n",
    "          writer=None,\n",
    "         ):\n",
    "    \"\"\" Trains adversarial networks for some number of epochs\n",
    "        :param D: (Discriminator) the discriminator network\n",
    "        :param G: (Generator) the generator network\n",
    "        :param n_epochs: (int) number of epochs to train for\n",
    "        :param print_every: (int) when to print and record the models' losses\n",
    "        :param sess: (dict) old session. dict or path to session dict or None: auto loads '.sess.json' file.\n",
    "                           To stop this, pass an empty dict\n",
    "        :param path: (str) path to work directory where checkpoints and logs will be dumped, defaults: \"runs/{data_name}/\"\n",
    "        return: D and G losses\"\"\"\n",
    "    import time, json, datetime\n",
    "\n",
    "    # move models to GPU\n",
    "    D.to(dev0)\n",
    "    G.to(dev1)\n",
    "\n",
    "    batch_size = train_loader.batch_size\n",
    "    batches_per_epoch = len(\n",
    "        train_loader.dataset.imgs) // train_loader.batch_size\n",
    "\n",
    "    print('batches_per_epoch:', batches_per_epoch)\n",
    "\n",
    "    sess = session.Sess({\n",
    "        'batch_size': batch_size,\n",
    "        'path': path,\n",
    "        'samples': [],  # keep track of loss and generated, \"fake\" samples\n",
    "        'D': {\n",
    "            'state_dict': D.state_dict(),\n",
    "            'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'losses': [],\n",
    "        },\n",
    "        'G': {\n",
    "            'state_dict': G.state_dict(),\n",
    "            'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'losses': [],\n",
    "        },\n",
    "        'z_fixed': Generator.gen_z(16, z_size).to(dev1),\n",
    "        'epoch': 0,  # completed epochs\n",
    "        'batch': 0,  # completed batches\n",
    "        'alltime_start_time': time.time(),\n",
    "        'print_every': print_every,\n",
    "        'save_every': save_every,\n",
    "    })\n",
    "\n",
    "    ## Load checkpoint\n",
    "    sess.load(ckpt)\n",
    "\n",
    "    D.load_state_dict(sess['D']['state_dict'])\n",
    "    #         d_optimizer.load_state_dict(sess['D']['optimizer_state_dict'])\n",
    "    G.load_state_dict(sess['G']['state_dict'])\n",
    "    #         g_optimizer.load_state_dict(sess['G']['optimizer_state_dict'])\n",
    "\n",
    "    # Get some fixed data for sampling. These are images that are held\n",
    "    # constant throughout training, and allow us to inspect the model's performance\n",
    "\n",
    "    sess['start_time'] = time.time()\n",
    "    n_epochs += sess['epoch']\n",
    "\n",
    "    # epoch training loop\n",
    "    for epoch in range(sess['epoch'], n_epochs, 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # batch training loop\n",
    "        for batch_i, (real_images, labels) in enumerate(train_loader):\n",
    "            real_images = utils.scale(real_images).to(dev0)\n",
    "            # labels = labels.to(dev0)\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            # ============================================\n",
    "            #            TRAIN THE DISCRIMINATOR\n",
    "            # ============================================\n",
    "            # 1. Train with REAL images\n",
    "            d_out_r = D(real_images)\n",
    "\n",
    "            # Compute the discriminator losses on REAL images (use smoothed labels)\n",
    "            #!!!! TODO: use labels!!!!\n",
    "            d_loss_r = real_loss(d_out_r, smooth=False)\n",
    "\n",
    "            # 2. Train with fake images\n",
    "            # Generate fake images\n",
    "            fake_images = G.generate(batch_size)\n",
    "\n",
    "            # Compute the discriminator losses on FAKE images\n",
    "            d_out_f = D(fake_images)\n",
    "            d_loss_f = fake_loss(d_out_f)\n",
    "\n",
    "            # add up real and fake losses and perform backprop\n",
    "            d_loss = d_loss_f + d_loss_r\n",
    "\n",
    "            # retain_graph=True so that we can reuse this info for the generator\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # =========================================\n",
    "            #            TRAIN THE GENERATOR\n",
    "            # =========================================\n",
    "\n",
    "            # 1. Train with fake images and flipped labels\n",
    "\n",
    "            # Generate fake images.  (we already generated those images up there,)\n",
    "            # Compute the discriminator losses on fake images.  using flipped labels!\n",
    "\n",
    "            # G loss, is how likely were the fake images believable by D?\n",
    "            g_loss = real_loss(d_out_f)\n",
    "\n",
    "            # perform backprop\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # Print some loss sess\n",
    "            if batch_i % sess['print_every'] == 0:\n",
    "                # append discriminator loss and generator loss\n",
    "                sess['G']['losses'].append(g_loss.item())\n",
    "                sess['D']['losses'].append(d_loss.item())\n",
    "\n",
    "                # print discriminator and generator loss\n",
    "                print(\n",
    "                    'Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.\n",
    "                    format(epoch + 1, n_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "            if batch_i % sess['save_every'] == 0 or (batch_i == 0\n",
    "                                                     and epoch == 0):\n",
    "                ## save checkpoint\n",
    "                dict_update(\n",
    "                    sess, {\n",
    "                        'epoch': epoch,\n",
    "                        'batch': batch_i,\n",
    "                        'D': {\n",
    "                            'state_dict': D.state_dict(),\n",
    "                            'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                        },\n",
    "                        'G': {\n",
    "                            'state_dict': G.state_dict(),\n",
    "                            'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                        },\n",
    "                    })\n",
    "\n",
    "                torch.save(\n",
    "                    sess,\n",
    "                    os.path.join(path, f'checkpoints/checkpoint_{epoch}.ckpt'))\n",
    "\n",
    "                # ================================================================== #\n",
    "                #                        Tensorboard Logging                         #\n",
    "                # ================================================================== #\n",
    "                models_losses = {'D': (D, d_loss), 'G': (G, g_loss)}\n",
    "                for model_name, (model, loss) in models_losses.items():\n",
    "                    # 1. Log scalar values (scalar summary)\n",
    "                    for tag, value in ({'loss': loss.item()}).items():\n",
    "                        writer.add_scalar(model_name + '/' + tag, value, epoch)\n",
    "\n",
    "                    # 2. Log values and gradients of the parameters (histogram summary)\n",
    "                    for tag, value in model.named_parameters():\n",
    "                        tag = tag.replace('.', '/')\n",
    "                        writer.add_histogram('/'.join([model_name, tag]),\n",
    "                                             value.data.cpu(), epoch)\n",
    "                        writer.add_histogram(\n",
    "                            '/'.join([model_name, tag, 'grad']),\n",
    "                            value.grad.data.cpu(), epoch)\n",
    "                pass\n",
    "                # optimization\n",
    "                #                 del g_loss\n",
    "                #                 del d_loss\n",
    "\n",
    "            # once per epoch\n",
    "            if batch_i == 0:\n",
    "                # 3. Log training images (image summary)\n",
    "                # make fixed fakes\n",
    "                sample = G.eval().forward(sess['z_fixed'])\n",
    "                G.train()  # back to training mode\n",
    "\n",
    "                sess['samples'].append(sample)\n",
    "\n",
    "                images = np.array(list(map(utils.unscale, sample)),\n",
    "                                  dtype=np.uint8) * -1\n",
    "                for tag, value in ({'generated images': images}).items():\n",
    "                    writer.add_images(tag, value, epoch)\n",
    "\n",
    "                fig, axes = utils.view_samples(sample, epoch)\n",
    "                plt.savefig(\n",
    "                    os.path.join(path, f'samples/samples (epoch {epoch}).png'))\n",
    "                plt.show()\n",
    "            pass  #end of batch\n",
    "\n",
    "        ## AFTER EACH EPOCH ##\n",
    "        if epoch == 0:  # only the first epoch: print expected time needed\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            expected_duration_remaining = epoch_duration * (n_epochs - 1)\n",
    "\n",
    "            later = datetime.datetime.now() + datetime.timedelta(\n",
    "                expected_duration_remaining)\n",
    "            # dd/mm/YY H:M:S\n",
    "            print(\n",
    "                f'Estimated time needed:\\t{utils.elapsed_time(expected_duration_remaining)}.'\n",
    "                f'\\tCome back at around:\\t{later.strftime(\"%d/%m/%Y %I:%M%p\")}'\n",
    "                f'\\nepoch takes {utils.elapsed_time(epoch_duration)}')\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "        print('time taken:',\n",
    "              utils.elapsed_time(time.time() - sess['start_time']))\n",
    "\n",
    "        return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:21:18.007985Z",
     "start_time": "2019-12-24T12:21:10.552Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "ckpt = train_sess = train(\n",
    "    D,\n",
    "    G,\n",
    "    n_epochs=n_epochs,\n",
    "    print_every=150,\n",
    "    save_every=50,\n",
    "    device=device,\n",
    "    path=f'runs/{data_name}',\n",
    "    ckpt=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = utils.load_ckpt(\n",
    "    os.path.join(run_dir, 'checkpoints'),\n",
    "    ckpt=25\n",
    ")\n",
    "\n",
    "print('loaded last checkpoint.\\nkeys:', list(ckpt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from checkpoint\n",
    "D.load_state_dict(ckpt['D']['state_dict'])\n",
    "G.load_state_dict(ckpt['G']['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loss\n",
    "\n",
    "Plot the training losses for the generator and discriminator, recorded after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:39.313562Z",
     "start_time": "2019-12-24T12:20:34.152Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(ckpt['D']['losses'], label='Discriminator', alpha=0.5)\n",
    "plt.plot(ckpt['G']['losses'], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "\n",
    "prints_per_epoch = round(2 * ckpt['print_every'] / (ckpt['batch_size'] + 1))\n",
    "\n",
    "print(prints_per_epoch)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generator samples from training\n",
    "\n",
    "View samples of images from the generator, and answer a question about the strengths and weaknesses of your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T12:20:39.314561Z",
     "start_time": "2019-12-24T12:20:34.154Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_samples_for_epochs(start_end=(-6, -1)):\n",
    "    start, end = start_end\n",
    "    start, end = start%len(ckpt['samples']), end%len(ckpt['samples'])\n",
    "    print(f'Showing samples from epoch {start} to epoch {end}')\n",
    "    for i in range(start, end, 1):\n",
    "        f, ax = utils.view_samples(ckpt['samples'][i], i)\n",
    "    plt.show()\n",
    "\n",
    "ipywidgets.interact_manual(show_samples_for_epochs, start_end=ipywidgets.IntRangeSlider(\n",
    "    value=[ckpt['epoch'] - 5, ckpt['epoch']],\n",
    "    min=0,\n",
    "    max=ckpt['epoch'],\n",
    "    step=1,\n",
    "    description='Epoch range',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "));\n",
    "\n",
    "# show_samples_for_epochs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate(n=80):\n",
    "    # generating new random samples (not from the fixed samples)\n",
    "    G.to(device).eval()\n",
    "    start_time = time.time()\n",
    "    sample = G.generate(n)\n",
    "    print('time taken to generate {} samples: {:.3f} seconds'.format(n, time.time()-start_time))\n",
    "    fig, axes = utils.view_samples(sample)\n",
    "    # plt.savefig(os.path.join(path, f'samples/samples (epoch {epoch}).png'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ipywidgets.interact_manual(generate, n=ipywidgets.IntSlider(\n",
    "    value=80,\n",
    "    min=0,\n",
    "    max=800,\n",
    "    step=8,\n",
    "    description='n samples',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "));\n",
    "\n",
    "# generate(80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on generated samples, and steps to improve this model\n",
    "* The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It did learn to generate things that look like faces, they all have eyes, mouths, and noses, but they still aren't realistic enough.\n",
    "Now the brute force approach would be to to make a larger model and train for much longer.\n",
    "\n",
    "But smarter thing to do is to try tweeking the hypterparameters"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (deep-learning-v2-pytorch)",
   "language": "python",
   "name": "pycharm-8389e0fb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "587.4px",
    "left": "1602.6px",
    "right": "20px",
    "top": "108px",
    "width": "357.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
